impacts$obj_key <- sapply(impacts$`Object Name`, normalize_name)
orbits$obj_key  <- sapply(orbits$`Object Name`, normalize_name)
# Merge dataset
orbits_clean <- orbits %>% select(-`Object Name`)
df <- impacts %>%
left_join(orbits_clean, by = "obj_key", suffix = c("_imp", "_orb"))
cat("Merge success:", nrow(df), "rows,", ncol(df), "cols\n")
library(dplyr)
df <- df %>%
select(-any_of(leak_cols))
# daftar kolom leakage
leak_cols <- c(
"Cumulative Impact Probability",
"Cumulative Palermo Scale",
"Maximum Palermo Scale",
"Maximum Torino Scale"
)
# hapus kolom
df <- df[, !(names(df) %in% leak_cols)]
orbit_cols <- c(
"Orbit Axis (AU)", "Orbit Eccentricity", "Orbit Inclination (deg)",
"Minimum Orbit Intersection Distance (AU)",
"Perihelion Distance (AU)", "Aphelion Distance (AU)",
"Orbital Period (yr)"
)
# Hapus baris yang memiliki NA di kolom orbit
df <- df[complete.cases(df[, orbit_cols]), ]
library(dplyr)
df <- df %>%
mutate(
log_diameter = log1p(`Asteroid Diameter (km)`),
log_velocity = log1p(`Asteroid Velocity`),
log_impacts  = log1p(`Possible Impacts`),
`Impact Energy (Mt)` = if_else(
`"Impact Energy (Mt)"` %in% names(df),
`"Impact Energy (Mt)"`,
(`Asteroid Diameter (km)`^3 * (`Asteroid Velocity`^2))
),
log_energy = log1p(`Impact Energy (Mt)`)
)
# 1. log_diameter
df$log_diameter <- log1p(df[["Asteroid Diameter (km)"]])
# 2. log_velocity
df$log_velocity <- log1p(df[["Asteroid Velocity"]])
# 3. log_impacts
df$log_impacts  <- log1p(df[["Possible Impacts"]])
# 4. Impact Energy (jika belum ada → buat)
if (!("Impact Energy (Mt)" %in% names(df))) {
df[["Impact Energy (Mt)"]] <-
df[["Asteroid Diameter (km)"]]^3 * (df[["Asteroid Velocity"]]^2)
}
# 5. log_energy
df$log_energy <- log1p(df[["Impact Energy (Mt)"]])
cat("Log transformation selesai.\n")
# 1. Diameter × Velocity
df$diameter_x_velocity <- df[["Asteroid Diameter (km)"]] * df[["Asteroid Velocity"]]
# 2. Diameter × Possible Impacts
df$diameter_x_impacts <- df[["Asteroid Diameter (km)"]] * df[["Possible Impacts"]]
# 3. Velocity × Possible Impacts
df$velocity_x_impacts <- df[["Asteroid Velocity"]] * df[["Possible Impacts"]]
# 4. Velocity × MOID
df$velocity_x_MOID <- df[["Asteroid Velocity"]] * df[["Minimum Orbit Intersection Distance (AU)"]]
# 5. Diameter × MOID
df$diameter_x_MOID <- df[["Asteroid Diameter (km)"]] * df[["Minimum Orbit Intersection Distance (AU)"]]
# 6. Energy × MOID (buat energi jika belum ada)
if (!("Impact Energy (Mt)" %in% names(df))) {
df[["Impact Energy (Mt)"]] <- df[["Asteroid Diameter (km)"]]^3 * (df[["Asteroid Velocity"]]^2)
}
df$energy_x_MOID <- df[["Impact Energy (Mt)"]] * df[["Minimum Orbit Intersection Distance (AU)"]]
cat("Interaction features selesai dibuat.\n")
# Orbit eccentricity squared
df$ecc2 <- df[["Orbit Eccentricity"]] ^ 2
# Inclination squared
df$incl2 <- df[["Orbit Inclination (deg)"]] ^ 2
# Orbit axis squared
df$axis2 <- df[["Orbit Axis (AU)"]] ^ 2
# Velocity squared
df$velocity2 <- df[["Asteroid Velocity"]] ^ 2
# Diameter squared
df$diameter2 <- df[["Asteroid Diameter (km)"]] ^ 2
# MOID squared
df$moid2 <- df[["Minimum Orbit Intersection Distance (AU)"]] ^ 2
cat("Polynomial features (degree 2) selesai dibuat.\n")
# 1. Perihelion / Aphelion ratio
df$peri_to_aphe_ratio <- df[["Perihelion Distance (AU)"]] / df[["Aphelion Distance (AU)"]]
# 2. Sin dari inclination (konversi derajat → radian)
df$incl_sin <- sin(df[["Orbit Inclination (deg)"]] * pi / 180)
# 3. Cos dari inclination
df$incl_cos <- cos(df[["Orbit Inclination (deg)"]] * pi / 180)
# 4. Normalized perihelion distance
df$peri_norm <- df[["Perihelion Distance (AU)"]] / df[["Orbit Axis (AU)"]]
# 5. Normalized aphelion distance
df$aphe_norm <- df[["Aphelion Distance (AU)"]] / df[["Orbit Axis (AU)"]]
cat("Orbit dynamics features selesai dibuat.\n")
feature_list <- c(
# Log-transformed features
"log_diameter",
"log_velocity",
"log_impacts",
"log_energy",
# Orbit features (inti)
"Orbit Axis (AU)",
"Orbit Eccentricity",
"Orbit Inclination (deg)",
"Minimum Orbit Intersection Distance (AU)",
"Perihelion Distance (AU)",
"Aphelion Distance (AU)",
"Orbital Period (yr)",
# Interaction features
"diameter_x_velocity",
"diameter_x_impacts",
"velocity_x_impacts",
"velocity_x_MOID",
"diameter_x_MOID",
"energy_x_MOID",
# Polynomial features
"ecc2",
"incl2",
"axis2",
"velocity2",
"diameter2",
"moid2",
# Orbit dynamics features
"peri_to_aphe_ratio",
"incl_sin",
"incl_cos",
"peri_norm",
"aphe_norm"
)
# Pilih fitur yang benar-benar ada di df
available_features <- feature_list[feature_list %in% names(df)]
cat("Fitur akhir yang digunakan untuk X:\n")
for (f in available_features) {
cat("-", f, "\n")
}
# 1. Buat kolom HighRisk
df$HighRisk <- as.integer(
(df[["Possible Impacts"]] >= 10) |
((df[["Asteroid Diameter (km)"]] >= 0.05) &
(df[["Minimum Orbit Intersection Distance (AU)"]] < 0.01)) |
((df[["Impact Energy (Mt)"]] > median(df[["Impact Energy (Mt)"]], na.rm = TRUE)) &
(df[["Asteroid Velocity"]] > median(df[["Asteroid Velocity"]], na.rm = TRUE)))
)
# 2. Tampilkan distribusi
cat("Distribusi HighRisk:\n")
print(table(df$HighRisk))
# Buat X dan y
X <- df[, available_features, drop = FALSE]
y <- as.integer(df$HighRisk)
# Print shape
cat("Shape X:", nrow(X), "rows,", ncol(X), "cols\n")
cat("Shape y:", length(y), "\n")
# Total missing dalam X
total_missing <- sum(is.na(X))
cat("Total missing dalam X:", total_missing, "\n\n")
# Distribusi y
cat("Distribusi y:\n")
print(table(y))
library(caret)
set.seed(42)   # sama seperti random_state=42
# Stratified split seperti train_test_split(..., stratify=y)
idx_train <- createDataPartition(y, p = 0.80, list = FALSE)
X_train <- X[idx_train, , drop = FALSE]
X_test  <- X[-idx_train, , drop = FALSE]
y_train <- y[idx_train]
y_test  <- y[-idx_train]
cat("=== Split Completed ===\n")
cat("Train size :", nrow(X_train), "\n")
cat("Test size  :", nrow(X_test), "\n\n")
cat("Distribusi kelas di TRAIN:\n")
print(table(y_train))
cat("\nDistribusi kelas di TEST:\n")
print(table(y_test))
library(caret)
# buat scaler (hanya pada data training)
preproc <- preProcess(X_train, method = c("center", "scale"))
# scaling training set
X_train_scaled <- predict(preproc, X_train)
# scaling test set (pakai scaler yang sama)
X_test_scaled  <- predict(preproc, X_test)
cat("=== Scaling Done ===\n")
cat("X_train_scaled shape:", nrow(X_train_scaled), "rows,", ncol(X_train_scaled), "cols\n")
cat("X_test_scaled  shape:", nrow(X_test_scaled), "rows,", ncol(X_test_scaled), "cols\n")
library(smotefamily)
# Ubah y_train menjadi faktor (wajib untuk smotefamily)
y_train_factor <- as.factor(y_train)
# Gabungkan X dan y (smotefamily butuh 1 dataframe lengkap)
train_df <- data.frame(X_train_scaled, HighRisk = y_train_factor)
# Terapkan SMOTE + Tomek Links
smt_result <- SMOTE(train_df[, -ncol(train_df)],
train_df$HighRisk,
K = 5, dup_size = 0)
# Ambil hasil SMOTE
X_smote <- smt_result$data[, -ncol(smt_result$data)]
y_smote <- smt_result$data$HighRisk
# TOMEL LINK CLEANING
clean_result <- TomekClassif(X_smote, y_smote)
smote_out <- SMOTE(X_train_scaled, as.factor(y_train), K=5)
X_train_final <- smote_out$data[, -ncol(smote_out$data)]
y_train_final <- smote_out$data$Class
library(smotefamily)
# Ubah y_train menjadi faktor (wajib untuk smotefamily)
y_train_factor <- as.factor(y_train)
# Gabungkan X dan y (smotefamily butuh 1 dataframe lengkap)
train_df <- data.frame(X_train_scaled, HighRisk = y_train_factor)
# Terapkan SMOTE + Tomek Links
smt_result <- SMOTE(train_df[, -ncol(train_df)],
train_df$HighRisk,
K = 5, dup_size = 0)
# Ambil hasil SMOTE
X_smote <- smt_result$data[, -ncol(smt_result$data)]
y_smote <- smt_result$data$HighRisk
# TOMEL LINK CLEANING
clean_result <- TomekClassif(X_smote, y_smote)
library(smotefamily)
library(UBL)
install.packages("UBL")
install.packages("UBL")
library(smotefamily)
library(UBL)
library(smotefamily)
# Ubah y_train menjadi faktor (wajib untuk smotefamily)
y_train_factor <- as.factor(y_train)
# Gabungkan X dan y (smotefamily butuh 1 dataframe lengkap)
train_df <- data.frame(X_train_scaled, HighRisk = y_train_factor)
# Terapkan SMOTE + Tomek Links
smt_result <- SMOTE(train_df[, -ncol(train_df)],
train_df$HighRisk,
K = 5, dup_size = 0)
# Ambil hasil SMOTE
X_smote <- smt_result$data[, -ncol(smt_result$data)]
y_smote <- smt_result$data$HighRisk
# TOMEL LINK CLEANING
clean_result <- TomekClassif(X_smote, y_smote)
install.packages("UBL")   # jika belum pernah install
library(UBL)
library(smotefamily)
# Ubah y menjadi faktor
y_train_factor <- as.factor(y_train)
# Gabungkan X dan y
train_df <- data.frame(X_train_scaled, HighRisk = y_train_factor)
# Terapkan SMOTE
smote_result <- SMOTE(train_df[, -ncol(train_df)],
train_df$HighRisk,
K = 5)
# Ambil hasil SMOTE
X_train_final <- smote_result$data[, -ncol(smote_result$data)]
y_train_final <- smote_result$data$class
# Output distribusi
cat("Distribusi baru:\n")
print(table(y_train_final))
cat("Shape X_train_final:", nrow(X_train_final),
"rows,", ncol(X_train_final), "cols\n")
cat("Shape y_train_final:", length(y_train_final), "\n")
library(caret)       # untuk train(), trainControl(), confusion matrix, grid search
library(pROC)        # untuk ROC & AUC
library(dplyr)       # (optional) untuk data wrangling
model_lr <- train(
x = X_train_final,
y = factor(y_train_final, levels = c(0,1), labels = c("neg","pos")),
method = "glm",
family = "binomial",
trControl = trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary
),
control = glm.control(maxit = 500),
metric = "ROC"
)
library(glmnet)
# generate grid parameter
alpha_grid <- c(0, 1)        # 0=ridge, 1=lasso
lambda_grid <- 1 / c(0.01, 0.1, 1, 10, 100)  # C = 1/lambda
# class weights: balanced
weights_balanced <- ifelse(y_train_final == 1,
sum(y_train_final==0)/sum(y_train_final==1),
1)
# Grid Search untuk glmnet
X_mat <- as.matrix(X_train_final)
y_vec <- as.numeric(y_train_final)
best_auc <- -Inf
best_model <- NULL
best_alpha <- NULL
best_lambda <- NULL
for (alpha_val in alpha_grid) {
for (lambda_val in lambda_grid) {
# Train model
fit <- glmnet(
X_mat, y_vec,
family = "binomial",
alpha = alpha_val,     # 0=ridge, 1=lasso
lambda = lambda_val,   # sama dengan C=1/lambda
weights = weights_balanced
)
# Probabilitas prediksi pada training set
pred_prob <- predict(fit, X_mat, type = "response")
# Hitung AUC
auc_val <- pROC::auc(y_vec, pred_prob)
# Update jika model lebih baik
if (auc_val > best_auc) {
best_auc <- auc_val
best_model <- fit
best_alpha <- alpha_val
best_lambda <- lambda_val
}
}
}
cat("=== GRID SEARCH COMPLETED ===\n")
cat("Best AUC:", best_auc, "\n")
cat("Best alpha (0=ridge,1=lasso):", best_alpha, "\n")
cat("Best lambda:", best_lambda, "\n")
# Ubah X_test ke matrix (glmnet butuh matrix)
X_test_mat <- as.matrix(X_test_scaled)
# Probabilitas kelas positif (y_prob)
y_prob <- as.numeric(predict(best_model, X_test_mat, type = "response"))
# Prediksi kelas (y_pred)
y_pred <- ifelse(y_prob > 0.5, 1, 0)
library(pROC)
# 1. ROC TRAIN
y_prob_train <- as.numeric(predict(best_model, as.matrix(X_train_final), type = "response"))
roc_train <- roc(y_train_final, y_prob_train)
auc_train <- auc(roc_train)
# 2. ROC TEST
y_prob_test <- as.numeric(predict(best_model, as.matrix(X_test_scaled), type = "response"))
roc_test <- roc(y_test, y_prob_test)
auc_test <- auc(roc_test)
# 3. Plot ROC Train vs Test
plot(roc_train,
col = "#1f77b4",
lwd = 3,
main = "ROC Curve — Train vs Test",
legacy.axes = TRUE)
lines(roc_test,
col = "#ff7f0e",
lwd = 3,
lty = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)
legend("bottomright",
legend = c(
paste0("Train AUC = ", round(auc_train, 3)),
paste0("Test AUC  = ", round(auc_test, 3))
),
col = c("#1f77b4", "#ff7f0e"),
lwd = 3,
lty = c(1, 2)
)
library(pROC)
# -----------------------
# ROC TRAIN
# -----------------------
y_prob_train <- as.numeric(predict(best_model, as.matrix(X_train_final), type="response"))
roc_train <- roc(y_train_final, y_prob_train)
auc_train <- auc(roc_train)
# -----------------------
# ROC TEST
# -----------------------
y_prob_test <- as.numeric(predict(best_model, as.matrix(X_test_scaled), type="response"))
roc_test <- roc(y_test, y_prob_test)
auc_test <- auc(roc_test)
# -----------------------
# PLOT ROC KEREN
# -----------------------
par(mar=c(5,5,4,8))   # margin agar legend tidak menabrak plot
plot(
roc_train,
col = "#1f77b4",
lwd = 3,
legacy.axes = TRUE,
xlim = c(0,1), ylim = c(0,1),
main = "ROC Curve — Train vs Test",
cex.lab = 1.2,
cex.main = 1.4,
cex.axis = 1.1
)
lines(
roc_test,
col = "#ff7f0e",
lwd = 3,
lty = 2
)
abline(a = 0, b = 1, col = "gray70", lwd = 2, lty = 2)
legend(
"bottomright",
inset = c(0.02, 0.02),
legend = c(
paste0("Train AUC = ", round(auc_train, 3)),
paste0("Test AUC  = ", round(auc_test, 3))
),
col = c("#1f77b4", "#ff7f0e"),
lwd = 3,
lty = c(1, 2),
cex = 1.1,
bg = "white",
box.lwd = 0.7
)
library(pROC)
# 1. ROC TRAIN
y_prob_train <- as.numeric(predict(best_model, as.matrix(X_train_final), type = "response"))
roc_train <- roc(y_train_final, y_prob_train)
auc_train <- auc(roc_train)
# 2. ROC TEST
y_prob_test <- as.numeric(predict(best_model, as.matrix(X_test_scaled), type = "response"))
roc_test <- roc(y_test, y_prob_test)
auc_test <- auc(roc_test)
# 3. Plot ROC Train vs Test
plot(roc_train,
col = "#1f77b4",
lwd = 3,
main = "ROC Curve — Train vs Test",
legacy.axes = TRUE)
lines(roc_test,
col = "#ff7f0e",
lwd = 3,
lty = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)
legend("bottomright",
legend = c(
paste0("Train AUC = ", round(auc_train, 3)),
paste0("Test AUC  = ", round(auc_test, 3))
),
col = c("#1f77b4", "#ff7f0e"),
lwd = 3,
lty = c(1, 2)
)
library(caret)
library(ggplot2)
cat("\n=== CONFUSION MATRIX ===\n")
# Confusion matrix angka
cm <- table(
Actual = y_test,
Predicted = y_pred
)
print(cm)
# Ubah menjadi data frame untuk ggplot
cm_df <- as.data.frame(cm)
colnames(cm_df) <- c("Actual", "Predicted", "Freq")
# Plot heatmap confusion matrix
ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), color = "black", size = 6) +
scale_fill_gradient(low = "#DCE6F2", high = "#08306B") +
labs(
title = "Confusion Matrix Heatmap",
x = "Predicted Class",
y = "Actual Class"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
axis.text = element_text(size = 12),
legend.position = "right"
)
cat("\n=== CLASSIFICATION REPORT (SKLEARN FORMAT) ===\n")
precision_0 <- cm$byClass["Neg Pred Value"]
library(caret)
cat("\n=== CLASSIFICATION REPORT ===\n")
# Buat faktor dengan level 0 dan 1
pred_factor <- factor(y_pred, levels = c(0,1))
test_factor <- factor(y_test, levels = c(0,1))
# Confusion Matrix + Metrics
cm <- confusionMatrix(pred_factor, test_factor, positive = "1")
print(cm)
library(pROC)
cat("\n=== ROC-AUC (TEST SET) ===\n")
roc_test <- roc(y_test, y_prob)
auc_test <- auc(roc_test)
cat("AUC:", auc_test, "\n")
library(glmnet)
library(dplyr)
# ambil koefisien dari model terbaik (glmnet)
coef_raw <- coef(best_model)
# ubah ke vector biasa (drop sparse format)
coef_vec <- as.numeric(coef_raw)
# glmnet menyimpan intercept sebagai koefisien pertama
intercept <- coef_vec[1]
coef_values <- coef_vec[-1]   # sisakan koef fitur saja
# membuat dataframe koefisien fitur
coef_df <- data.frame(
feature = available_features,
coef = coef_values
) %>%
arrange(desc(coef))
cat("=== Koefisien Fitur Logistic Regression (glmnet) ===\n")
print(coef_df)
knitr::opts_chunk$set(echo = TRUE)
library(smotefamily)
library(ggplot2)
library(dplyr)
# =========================
# 1) SEBELUM SMOTE
# =========================
y_train_factor <- as.factor(y_train)
knitr::opts_chunk$set(echo = TRUE)
install.packages(c(
"tidyverse",  # dplyr, readr, stringr, dll (manipulasi data)
"caret",      # train-test split, scaling, evaluasi model
"glmnet",     # logistic regression dengan regularisasi (mirip penalty L1/L2 di Python)
"pROC"        # hitung AUC & plot ROC
))
install.packages("smotefamily")  # untuk SMOTE (nanti bisa ditambah)
# Load package untuk membaca CSV
library(readr)
# Path file dataset (sesuaikan dengan laptopmu)
PATH_IMPACTS <- "D:/KULIAH UNNES/Komputasi Statistika/Possible Asteroid Impacts with Earth/archive/impacts.csv"
PATH_ORBITS  <- "D:/KULIAH UNNES/Komputasi Statistika/Possible Asteroid Impacts with Earth/archive/orbits.csv"
# Load data
impacts <- read_csv(PATH_IMPACTS, show_col_types = FALSE)
install.packages("smotefamily")
orbits  <- read_csv(PATH_ORBITS,  show_col_types = FALSE)
install.packages(c("tidyverse", "caret", "glmnet", "pROC"))
